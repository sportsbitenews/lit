# Implications for HCI

I argue that there is a synergy between the study of trust and Human Computer Interaction (HCI), especially the information systems focus within HCI. HCI as a relatively new discipline, not only observes the ways humans interact with technology, but also designs technologies that can let people interact with technology in new ways. These two focuses, observing and designing can both influence and be informed by the understanding of trust.

## Observing

First, I would like to illustrate the usefulness of information technology in helping us better understand trust by contrasting two previous studies: the 1978 experiment by Cook and Emerson and the 1995 experiment by Berg. The 1978 experiment by Cook and Emerson {{"cook1978power" | cite}} was conducted using "a newly developed computerized laboratory", which "consists of eight isolated booth containing CRT computer terminals linked together through a PRIME 300 minicomputer". The study had a total of 112 subjects divided into four-person networks and interacted through the computer terminals performing 40 transaction decisions each, resulting in 4,480 data points in total. This is to contrast the trust experiments done in 1995 by Berg {{"berg1995trust" | cite}} without the aid of computers using a double blind procedure implemented with envelops, rooms, and mail boxes. The experiment ran two rounds (each round lasted more than three days), with 32 and 28 two-person pairs engaging in one transaction decisions, resulting in in 60 data points in total, far less than the one that was conducted with the assistance of computers.

Almost forty years later since the 1978 experiment, information technology present us not only the ability to collect new data at scale with ease, but also passively recording behavior data in the field naturally. We have enormous amount of online reviews, ratings, and cookies to examine empirically if certain language cues or visual display will lead to more or less trust in a potential buyer. We also have fine-grained data on daily communications among co-workers in emails or groupware such as Slack. On a interpersonal level, we have data on social tie strength that could allow augmented data analysis on the relationship between tie strength and trust (a 2011 study found that unit wall-post made on a friend's wall on Facebook is correlated with 2.3\% increase in investment in non-anonymous single-shot trust game {{"bapna2011trust" | cite}}).

Finally and fundamentally, information technology also introduced new elements in social structure, which by Seligman's view, will also change the nature of trust. Computer-Mediated-Communication (CMC) introduced anonymity and lack of visual cues that resulted in a hyperpersonal model and the disembodied nature of communication. Information technology has also changed our social behaviors {{"turkle2012alone" | cite}}: how we live {{"ikkala2015monetizing" | cite}}, work {{"friedman2014workers" | cite}}, and even, how we find love {{"ellison2006managing" | cite}}. As David Good wrote in his essay *Individuals, Interpersonal Relations, and Trust* {{"gambetta1988trust" | cite}}:"without trust, the everyday social life which we take for granted is simply not possible". Equally is true that without information systems, the everyday social life which we take for granted today is simply not possible. To ignore technology in the study of trust is equivalent to ignoring the ongoing changes in social relationships, which is not consistent with what early sociologists set out to do.

There has been some very interesting work on the intersection of information technology and trust. For example, several fascinating experimental studies and field studies compiled in the book eTrust: Forming Relationships in the Online World* {{"cheshire2009etrust" | cite}}, as well as some recent work by Ko Kuwabara at Columbia at Columbia Business School and his collaborators {{"kuwabara2014trust, kuwabara2015reputation" | cite}}, and more futuristically looking micro expression analysis and human robotic interaction by social psychologist David DeSteno and collaborators {{"desteno2012detecting" | cite}}. I argue these studies could be better understood when considered under the framework of information view of trust and its function in the social systems rather than viewed isolated.

There are also many unanswered questions at this intersection. For example, is trust situational or contextual? As the division of labour in our society continues to increase forming new types of organizations (see ["gig economy"](http://www.economist.com/blogs/democracyinamerica/2011/09/labour-markets), the model we use to account trust -- A trust B to do x -- requires a more careful examination on different actions of x. Can the action, x, be exhaustively enumerated? Does the "gig economy" make the trust around different actions of x less coupled with one another? Is there such thing as a global trust score, as some companies attempted to do (see [TrustCloud](http://www.huffingtonpost.com/sam-fiorella/linkedin-trustcloud-klout_b_2398201.html) and meta trust question associated with it ["How trustworthy is TrustCloud](https://www.quora.com/How-trustworthy-is-TrustCloud) and more recently, [Peeple](https://en.wikipedia.org/wiki/Peeple_(mobile_application)), or is it possible that the nature of x (e.g.: typical tasks or high-reliability tasks {{"colquitt2011trust" | cite}}) could moderate the relationship between cognitive and emotional trust and the behavior decisions it results?

Parenting relationship hasn't been really changed yet;?

## Designing

The second component of the synergy between trust and HCI is the potential to implement real-world solutions that foster more efficient social systems. If we follow Luhmann's view that trust is a mechanism to reduce complexity in social systems, it is clear that trust is not the *only* way. As we discussed, Barber has pointed out that rational distrust, law, and many other mechanisms are complement to trust in maintaining social order. I argue that the designing component of HCI could aid humans to reduce complexity in addition to trust, following the argument by Bruno Latour that *technology is society made durable* {{"latour1990technology" | cite}}. This could be done in four ways: reputation systems to reduce uncertainty, awareness applications to produce trust, context aware matching to reduce friction, and smart contract to provide additional social control. I will briefly describe each of these components below.

## Reputation Systems to Reduce Uncertainty

I use the term reputation system to refer to both the informal reputation systems that forms though word of mouth, as well as formal reputation systems that record and display ratings and reviews. A great survey paper on reputation systems in 2007 {{"josang2007survey" | cite}} gave an overview of existing reputation systems, and pointed out that they are "collaborative sanctioning systems". The paper proposed three of reputation system: long lived entities; ratings about current interactions are captured and distributed; and ratings for past interactions guide decisions of current interactions; and also pointed out problems that are associated with reputation systems, including "unfair ratings and ballot stuffing" {{"josang2007survey" | cite}}. These problems reputation systems suffer have deep sociological reasons such as power dependence {{"state2016" | cite}}, and are also closely related with the discussions of the boundary of information (privacy {{"nissenbaum2004privacy" | cite}}).

I think the study of reputation systems should not be isolated from the psychological component of trust. How does profile elements, such as photos, the degree of self-disclosure, language style, influence how people form impressions online? What signals do people base their "expectations" on? When is homophily good and when does it become toxic? How does familiarity compared to reputation? Can a well designed reputation systems correct bias or are they inherently biased because people's ratings are fundamentally subjective and bias-prone? Can we design the system that adjust the ratio of weights of objective metrics together with subjective information? These are the questions around reputation systems that I consider worth addressing.

## Awareness Applications to Produce Trust

According to Barber, trust provides "cognitive and moral expectational maps for actors and systems as they continuously interact" {{"barber1983logic" | cite}}. He did not specify, however, how such maps were formed and kept by communities. Is it purely based on word of mouth, anecdote, parenting? Or is trust produced through voluntary associations such as bowling leagues as Putnam argued {{"putnam2000bowling" | cite}}? Uslaner showed evidence that group membership did not produce trust {{"uslaner2000producing" | cite}}. We then ask the question does awareness produce trust? Can information applications, such as maps, visualizations, highlight certain aspects of the community that can nudge {{"thaler2008" | cite}} people form different maps? Will increasing awareness {{"dourish1992awareness" | cite}} be linked to increased trust?

## Context Aware Matching to Reduce Friction

A lot of trust scholars have focused on that trust is often referred to as "an important lubricant of a social system" {{"arrow1974limits" | cite}}. The same description has also been applied to identity information {{"ellison2011connection" | cite}}, non-verbal communication {{"burgoon1994interpersonal" | cite}}, and wine {{"charters2005wine" | cite}}. The point here is that trust is not the *sole* mechanism that can reduce social friction.

I argue that good recommendation systems also help reduce friction cost and can be a supplement towards the mechanism of trust. A ongoing NSF funded project, [Smart, Context-Aware, Peer-to-Peer Service and transaction Matching](http://www.victoriabellotti.com/peer-to-peer-transaction-matching.html) led by Victoria Bellotti at PARC, is a great example for this line of work. Another example will be to use available personal data to match people with volunteering opportunities (e.g., <http://www.nycservice.org>) for communities that they may trust more.

## Smart Contract as Additional Social Control

Trust alone is not enough for effective social control. With advancement in technologies such as cryptography, block chain, facial recognition, and other domains in artificial intelligence, it is not entirely crazy to envision these technology-enabled contracts that perform and sanction according to pre-defined rules that provides additional assurance.

What we do not fully understand yet is the relationship between interpersonal trust and assurance. It seems plausible that a curvilinear relationship may exist between the two, with two extremes being complete assurance -- no need to trust; no assurance -- faith, hope, confidence, and belief. There seems always have to be the element of risk and vulnerability {{"cook2005trust, cheshire2011online" | cite}} for trust to matter or to be produced. The enforcement of contracts can sometimes undermine trust {{"kuwabara2015reputation" | cite}}. This tension between trust and assurance is summarized neatly in one of my favorite quotes from Lewis and Weigert:"Trust begins where prediction ends." {{"lewis1985trust" | cite}}

## References
{% references %} {% endreferences %}
